{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(372.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#生成8对fp8浮点数\n",
    "e4m3_type = torch.float8_e4m3fnuz\n",
    "a0 = torch.tensor(2.0, dtype=e4m3_type)\n",
    "a1 = torch.tensor(3.0, dtype=e4m3_type)\n",
    "a2 = torch.tensor(4.0, dtype=e4m3_type)\n",
    "a3 = torch.tensor(5.0, dtype=e4m3_type)\n",
    "a4 = torch.tensor(6.0, dtype=e4m3_type)\n",
    "a5 = torch.tensor(7.0, dtype=e4m3_type)\n",
    "a6 = torch.tensor(8.0, dtype=e4m3_type)\n",
    "a7 = torch.tensor(9.0, dtype=e4m3_type)\n",
    "b0 = torch.tensor(4.0, dtype=e4m3_type)\n",
    "b1 = torch.tensor(5.0, dtype=e4m3_type)\n",
    "b2 = torch.tensor(6.0, dtype=e4m3_type)\n",
    "b3 = torch.tensor(7.0, dtype=e4m3_type)\n",
    "b4 = torch.tensor(8.0, dtype=e4m3_type)\n",
    "b5 = torch.tensor(9.0, dtype=e4m3_type)\n",
    "b6 = torch.tensor(10.0, dtype=e4m3_type)\n",
    "b7 = torch.tensor(11.0, dtype=e4m3_type)\n",
    "#计算8对fp8浮点数的乘积\n",
    "c0 = a0.to(torch.float32) * b0.to(torch.float32)\n",
    "c1 = a1.to(torch.float32) * b1.to(torch.float32)\n",
    "c2 = a2.to(torch.float32) * b2.to(torch.float32)\n",
    "c3 = a3.to(torch.float32) * b3.to(torch.float32)\n",
    "c4 = a4.to(torch.float32) * b4.to(torch.float32)\n",
    "c5 = a5.to(torch.float32) * b5.to(torch.float32)\n",
    "c6 = a6.to(torch.float32) * b6.to(torch.float32)\n",
    "c7 = a7.to(torch.float32) * b7.to(torch.float32)\n",
    "sum_c = c0 + c1 + c2 + c3 + c4 + c5 + c6 + c7\n",
    "print(sum_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign: 0, Exp: 0, Mant: 2\n",
      "Binary: 00000010\n",
      "Sign: 0, Exp: 0, Mant: 4\n",
      "Binary: 00000100\n",
      "Sign: 0, Exp: 1, Mant: 0\n",
      "Binary: 00001000\n"
     ]
    }
   ],
   "source": [
    "def print_fp8_binary(value):\n",
    "    # 将FP8值转换为整数\n",
    "    value_int = int(value.item())\n",
    "    # 或者使用 value_int = value.item().__int__()\n",
    "    \n",
    "    # 提取符号位、指数位和尾数位\n",
    "    sign_bit = (value_int >> 7) & 1\n",
    "    exp_bit = (value_int >> 3) & 0xF\n",
    "    mant_bit = value_int & 0x7\n",
    "    \n",
    "    print(f\"Sign: {sign_bit}, Exp: {exp_bit}, Mant: {mant_bit}\")\n",
    "    # 打印二进制表示\n",
    "    print(f\"Binary: {bin(value_int)[2:].zfill(8)}\")\n",
    "\n",
    "# 使用函数\n",
    "print_fp8_binary(a0)\n",
    "print_fp8_binary(b0)\n",
    "print_fp8_binary(c0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
